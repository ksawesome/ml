{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c75841",
   "metadata": {},
   "source": [
    "# Block 4: Pseudoinverse, Least-Squares & Conditioning\n",
    "\n",
    "## ML Foundations — Phase 0, Module 1, Block 4\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **pseudoinverse** and **least-squares** problem lie at the heart of machine learning. Every linear regression, every PCA computation, and every gradient-based optimization involves solving systems that may be overdetermined, underdetermined, or ill-conditioned. Understanding how to solve these problems **stably** is critical for reliable ML.\n",
    "\n",
    "### Why This Matters for ML\n",
    "\n",
    "| Problem | Connection to Pseudoinverse/Conditioning |\n",
    "|---------|------------------------------------------|\n",
    "| **Linear Regression** | Least-squares solution $\\hat{\\beta} = (X^T X)^{-1} X^T y$ |\n",
    "| **Ridge Regression** | Regularized pseudoinverse for ill-conditioning |\n",
    "| **PCA** | SVD-based, sensitive to small singular values |\n",
    "| **Neural Network Training** | Hessian conditioning affects optimization |\n",
    "| **Feature Engineering** | Collinear features → ill-conditioned design matrix |\n",
    "| **Numerical Stability** | Condition number predicts output sensitivity |\n",
    "\n",
    "### Block Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand** the geometry of least-squares: projection onto column space\n",
    "2. **Derive** the Moore–Penrose pseudoinverse from SVD\n",
    "3. **Prove** that $A^+ b$ gives the minimum-norm least-squares solution\n",
    "4. **Quantify** conditioning via $\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}$\n",
    "5. **Implement** three solvers: SVD, Normal Equations, QR\n",
    "6. **Demonstrate** when each method fails under ill-conditioning\n",
    "7. **Apply** insights to ML problems (regression, collinearity, optimization)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Title & Overview](#Block-4:-Pseudoinverse,-Least-Squares-&-Conditioning)\n",
    "2. [Least-Squares Problem Setup](#Section-2:-Least-Squares-Problem-Setup)\n",
    "3. [Moore–Penrose Pseudoinverse from SVD](#Section-3:-Moore–Penrose-Pseudoinverse-from-SVD)\n",
    "4. [Conditioning & Sensitivity](#Section-4:-Conditioning-&-Sensitivity)\n",
    "5. [Normal Equations vs SVD Solve](#Section-5:-Normal-Equations-vs-SVD-Solve)\n",
    "6. [QR Factorization Solve](#Section-6:-QR-Factorization-Solve)\n",
    "7. [Coding: Three Solvers](#Section-7:-Coding-Three-Solvers)\n",
    "8. [Ill-Conditioned Experiments](#Section-8:-Ill-Conditioned-Experiments)\n",
    "9. [Practical ML Context](#Section-9:-Practical-ML-Context)\n",
    "10. [Decision Flowchart](#Section-10:-Decision-Flowchart)\n",
    "11. [Final Summary](#Section-11:-Final-Summary)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Block 2:** Eigenvalues, spectral theorem\n",
    "- **Block 3:** SVD, low-rank approximation, singular values\n",
    "- **Python:** NumPy, Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b229291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP: Import Libraries\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "# Plotting configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# NumPy print options\n",
    "np.set_printoptions(precision=8, suppress=True)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e944a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Least-Squares Problem Setup\n",
    "\n",
    "## 2.1 The Overdetermined System\n",
    "\n",
    "In many ML problems, we have more equations than unknowns:\n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{m \\times n}$ with $m > n$ (tall matrix), $x \\in \\mathbb{R}^n$, and $b \\in \\mathbb{R}^m$.\n",
    "\n",
    "**Example:** Linear regression with $m$ samples and $n$ features:\n",
    "- $A$ = design matrix (samples × features)\n",
    "- $x$ = coefficient vector\n",
    "- $b$ = target values\n",
    "\n",
    "When $m > n$, there's generally **no exact solution** — we can't pass a line through all points.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Residual Minimization\n",
    "\n",
    "Instead of solving exactly, we minimize the **residual**:\n",
    "\n",
    "$$\\min_x \\|Ax - b\\|_2^2 = \\min_x \\sum_{i=1}^m (a_i^T x - b_i)^2$$\n",
    "\n",
    "This is the **ordinary least-squares (OLS)** problem.\n",
    "\n",
    "**Why squared norm?**\n",
    "1. Differentiable (enables calculus-based solutions)\n",
    "2. Penalizes large errors more than small ones\n",
    "3. Maximum likelihood under Gaussian noise assumption\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Geometric Interpretation\n",
    "\n",
    "The least-squares solution has a beautiful geometric meaning:\n",
    "\n",
    "**Key Insight:** $\\hat{x}$ minimizes $\\|Ax - b\\|$ when $A\\hat{x}$ is the **orthogonal projection** of $b$ onto the column space of $A$.\n",
    "\n",
    "$$\\hat{b} = A\\hat{x} = \\text{proj}_{\\text{col}(A)}(b)$$\n",
    "\n",
    "**Residual is orthogonal:** $r = b - A\\hat{x} \\perp \\text{col}(A)$\n",
    "\n",
    "This means $A^T r = 0$, which leads to the **normal equations**:\n",
    "$$A^T(b - A\\hat{x}) = 0 \\implies A^T A \\hat{x} = A^T b$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 When Does a Unique Solution Exist?\n",
    "\n",
    "| Condition | Consequence |\n",
    "|-----------|-------------|\n",
    "| $\\text{rank}(A) = n$ (full column rank) | Unique least-squares solution |\n",
    "| $\\text{rank}(A) < n$ | Infinitely many solutions; choose minimum-norm |\n",
    "| $A^T A$ invertible | Can use normal equations directly |\n",
    "| $A^T A$ singular | Need pseudoinverse or regularization |\n",
    "\n",
    "**Key insight:** The solution depends on the **rank** and **conditioning** of $A$, not just its shape.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 The Projection Matrix\n",
    "\n",
    "When $A$ has full column rank, the projection onto $\\text{col}(A)$ is:\n",
    "\n",
    "$$P = A(A^T A)^{-1} A^T$$\n",
    "\n",
    "**Properties:**\n",
    "- $P^2 = P$ (idempotent)\n",
    "- $P^T = P$ (symmetric)\n",
    "- $Pb = A\\hat{x}$ (projects $b$ onto column space)\n",
    "- $I - P$ projects onto the orthogonal complement (null space of $A^T$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e896d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 2.1: Geometric Least-Squares Visualization\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simple 2D example: fit a line through 3 points\n",
    "# Model: y = ax + b, or [x, 1] @ [a, b]^T = y\n",
    "\n",
    "# Three points that don't lie on a line\n",
    "points = np.array([[1, 2], [2, 3], [3, 3.5]])\n",
    "x_data, y_data = points[:, 0], points[:, 1]\n",
    "\n",
    "# Design matrix A (with intercept column)\n",
    "A = np.column_stack([x_data, np.ones(3)])\n",
    "b = y_data\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Exercise 2.1: Geometric Least-Squares\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDesign matrix A:\\n{A}\")\n",
    "print(f\"\\nTarget vector b: {b}\")\n",
    "\n",
    "# Solve via normal equations\n",
    "AtA = A.T @ A\n",
    "Atb = A.T @ b\n",
    "x_hat = np.linalg.solve(AtA, Atb)\n",
    "print(f\"\\nLeast-squares solution [slope, intercept]: {x_hat}\")\n",
    "\n",
    "# Compute projection and residual\n",
    "b_hat = A @ x_hat  # Projection onto col(A)\n",
    "residual = b - b_hat\n",
    "\n",
    "print(f\"\\nProjection Â x̂ = {b_hat}\")\n",
    "print(f\"Residual r = b - Ax̂ = {residual}\")\n",
    "print(f\"Residual norm: {np.linalg.norm(residual):.6f}\")\n",
    "\n",
    "# Verify orthogonality: A^T r = 0\n",
    "At_r = A.T @ residual\n",
    "print(f\"\\nOrthogonality check (A^T r ≈ 0): {At_r}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Data and fitted line\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(x_data, y_data, s=100, c='blue', label='Data points', zorder=5)\n",
    "x_line = np.linspace(0.5, 3.5, 100)\n",
    "y_line = x_hat[0] * x_line + x_hat[1]\n",
    "ax1.plot(x_line, y_line, 'r-', linewidth=2, label=f'Fit: y = {x_hat[0]:.2f}x + {x_hat[1]:.2f}')\n",
    "\n",
    "# Show residuals as vertical lines\n",
    "for i in range(3):\n",
    "    ax1.plot([x_data[i], x_data[i]], [y_data[i], b_hat[i]], 'g--', linewidth=2)\n",
    "    ax1.scatter([x_data[i]], [b_hat[i]], s=50, c='red', marker='x', zorder=5)\n",
    "\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Least-Squares Line Fit')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Geometric view in R^3 (b space)\n",
    "ax2 = axes[1]\n",
    "ax2.set_xlim(-1, 5)\n",
    "ax2.set_ylim(-1, 5)\n",
    "\n",
    "# Show b and its projection\n",
    "ax2.annotate('', xy=(b_hat[0], b_hat[1]), xytext=(0, 0),\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "ax2.annotate('', xy=(b[0], b[1]), xytext=(0, 0),\n",
    "             arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "ax2.annotate('', xy=(b[0], b[1]), xytext=(b_hat[0], b_hat[1]),\n",
    "             arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "ax2.text(b[0]+0.1, b[1]+0.1, 'b', fontsize=14, color='blue')\n",
    "ax2.text(b_hat[0]-0.3, b_hat[1]+0.1, 'Ax̂', fontsize=14, color='red')\n",
    "ax2.text((b[0]+b_hat[0])/2+0.1, (b[1]+b_hat[1])/2, 'r', fontsize=14, color='green')\n",
    "\n",
    "ax2.set_xlabel('Component 1')\n",
    "ax2.set_ylabel('Component 2')\n",
    "ax2.set_title('Geometric View: b, projection Ax̂, residual r\\n(showing first 2 components)')\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ The residual r is orthogonal to the column space of A\")\n",
    "print(\"✓ This is the geometric essence of least-squares!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f1cd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Moore–Penrose Pseudoinverse from SVD\n",
    "\n",
    "## 3.1 Motivation\n",
    "\n",
    "The ordinary inverse $A^{-1}$ only exists for square, non-singular matrices. But we need a generalized inverse that works for:\n",
    "- Rectangular matrices (overdetermined/underdetermined systems)\n",
    "- Rank-deficient matrices\n",
    "- Singular matrices\n",
    "\n",
    "The **Moore–Penrose pseudoinverse** $A^+$ is this universal solution.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Definition via SVD\n",
    "\n",
    "**Theorem 3.1 (Pseudoinverse via SVD):**\n",
    "\n",
    "*Let $A \\in \\mathbb{R}^{m \\times n}$ have SVD $A = U \\Sigma V^T$ with $r = \\text{rank}(A)$ nonzero singular values $\\sigma_1 \\geq \\cdots \\geq \\sigma_r > 0$. Then the pseudoinverse is:*\n",
    "\n",
    "$$A^+ = V \\Sigma^+ U^T$$\n",
    "\n",
    "*where $\\Sigma^+ \\in \\mathbb{R}^{n \\times m}$ has diagonal entries:*\n",
    "\n",
    "$$(\\Sigma^+)_{ii} = \\begin{cases} 1/\\sigma_i & i \\leq r \\\\ 0 & i > r \\end{cases}$$\n",
    "\n",
    "**Explicit formula:**\n",
    "\n",
    "$$A^+ = \\sum_{i=1}^r \\frac{1}{\\sigma_i} v_i u_i^T$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Cases by Matrix Shape\n",
    "\n",
    "### Case 1: Tall matrix ($m > n$), full column rank\n",
    "\n",
    "$$A^+ = (A^T A)^{-1} A^T$$\n",
    "\n",
    "This gives the **least-squares** solution: $x = A^+ b$ minimizes $\\|Ax - b\\|$.\n",
    "\n",
    "### Case 2: Wide matrix ($m < n$), full row rank\n",
    "\n",
    "$$A^+ = A^T (A A^T)^{-1}$$\n",
    "\n",
    "This gives the **minimum-norm** solution among all exact solutions.\n",
    "\n",
    "### Case 3: Rank-deficient matrix\n",
    "\n",
    "Use SVD: $A^+ = V \\Sigma^+ U^T$. This gives the **minimum-norm least-squares** solution.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 The Four Moore–Penrose Conditions\n",
    "\n",
    "**Definition 3.2:** The pseudoinverse $A^+$ is the unique matrix satisfying:\n",
    "\n",
    "1. $A A^+ A = A$ (generalized inverse)\n",
    "2. $A^+ A A^+ = A^+$ (reflexive)\n",
    "3. $(A A^+)^T = A A^+$ (hermitian projection onto col(A))\n",
    "4. $(A^+ A)^T = A^+ A$ (hermitian projection onto row(A))\n",
    "\n",
    "**Proof that SVD formula satisfies these:**\n",
    "\n",
    "Let $A = U\\Sigma V^T$ and $A^+ = V\\Sigma^+ U^T$.\n",
    "\n",
    "**(1)** $A A^+ A = U\\Sigma V^T \\cdot V\\Sigma^+ U^T \\cdot U\\Sigma V^T = U\\Sigma\\Sigma^+\\Sigma V^T = U\\Sigma V^T = A$ ✓\n",
    "\n",
    "**(2)** $A^+ A A^+ = V\\Sigma^+ U^T \\cdot U\\Sigma V^T \\cdot V\\Sigma^+ U^T = V\\Sigma^+\\Sigma\\Sigma^+ U^T = V\\Sigma^+ U^T = A^+$ ✓\n",
    "\n",
    "**(3)** $(AA^+)^T = (U\\Sigma\\Sigma^+ U^T)^T = U\\Sigma\\Sigma^+ U^T = AA^+$ ✓ (since $\\Sigma\\Sigma^+$ is diagonal)\n",
    "\n",
    "**(4)** $(A^+A)^T = (V\\Sigma^+\\Sigma V^T)^T = V\\Sigma^+\\Sigma V^T = A^+A$ ✓\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Minimum-Norm Least-Squares Solution\n",
    "\n",
    "**Theorem 3.3 (Fundamental Theorem of Pseudoinverse):**\n",
    "\n",
    "*The solution $x^+ = A^+ b$ is the unique vector that:*\n",
    "\n",
    "1. *Minimizes $\\|Ax - b\\|$ (least-squares)*\n",
    "2. *Among all minimizers, has minimum $\\|x\\|$ (minimum-norm)*\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "**Part 1 (Least-squares):** Let $\\hat{b} = AA^+ b$ be the projection of $b$ onto $\\text{col}(A)$.\n",
    "\n",
    "$A x^+ = A A^+ b = U\\Sigma V^T V\\Sigma^+ U^T b = U\\Sigma\\Sigma^+ U^T b$\n",
    "\n",
    "For $i \\leq r$: $(\\Sigma\\Sigma^+)_{ii} = 1$, so $AA^+$ projects onto the first $r$ columns of $U$ = $\\text{col}(A)$.\n",
    "\n",
    "Thus $Ax^+ = \\text{proj}_{\\text{col}(A)}(b)$, which minimizes $\\|Ax - b\\|$.\n",
    "\n",
    "**Part 2 (Minimum-norm):** Any solution can be written as $x = x^+ + z$ where $z \\in \\text{null}(A)$.\n",
    "\n",
    "$\\|x\\|^2 = \\|x^+ + z\\|^2 = \\|x^+\\|^2 + 2\\langle x^+, z \\rangle + \\|z\\|^2$\n",
    "\n",
    "Now $x^+ = V\\Sigma^+ U^T b = \\sum_{i=1}^r \\frac{u_i^T b}{\\sigma_i} v_i$ lies in $\\text{row}(A) = \\text{span}\\{v_1, \\ldots, v_r\\}$.\n",
    "\n",
    "Since $z \\in \\text{null}(A) = \\text{span}\\{v_{r+1}, \\ldots, v_n\\}$ and these are orthogonal:\n",
    "\n",
    "$\\langle x^+, z \\rangle = 0$\n",
    "\n",
    "Therefore $\\|x\\|^2 = \\|x^+\\|^2 + \\|z\\|^2 \\geq \\|x^+\\|^2$, with equality iff $z = 0$. $\\square$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 3.1: Verify Moore-Penrose Conditions\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 3.1: Verify Moore-Penrose Conditions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test on various matrix types\n",
    "test_cases = {\n",
    "    \"Tall full-rank (5×3)\": np.random.randn(5, 3),\n",
    "    \"Wide full-rank (3×5)\": np.random.randn(3, 5),\n",
    "    \"Rank-deficient (5×4, rank 2)\": np.random.randn(5, 2) @ np.random.randn(2, 4),\n",
    "    \"Square singular (4×4)\": np.column_stack([np.random.randn(4, 2), np.random.randn(4, 2) @ [[1, 0], [0, 0]]])\n",
    "}\n",
    "\n",
    "for name, A in test_cases.items():\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Case: {name}\")\n",
    "    print(f\"Shape: {A.shape}, Rank: {np.linalg.matrix_rank(A)}\")\n",
    "    \n",
    "    # Compute pseudoinverse via NumPy\n",
    "    A_pinv = np.linalg.pinv(A)\n",
    "    \n",
    "    # Verify four Moore-Penrose conditions\n",
    "    cond1 = np.linalg.norm(A @ A_pinv @ A - A)\n",
    "    cond2 = np.linalg.norm(A_pinv @ A @ A_pinv - A_pinv)\n",
    "    cond3 = np.linalg.norm((A @ A_pinv).T - A @ A_pinv)\n",
    "    cond4 = np.linalg.norm((A_pinv @ A).T - A_pinv @ A)\n",
    "    \n",
    "    print(f\"  (1) ||AA⁺A - A|| = {cond1:.2e}\")\n",
    "    print(f\"  (2) ||A⁺AA⁺ - A⁺|| = {cond2:.2e}\")\n",
    "    print(f\"  (3) ||(AA⁺)ᵀ - AA⁺|| = {cond3:.2e}\")\n",
    "    print(f\"  (4) ||(A⁺A)ᵀ - A⁺A|| = {cond4:.2e}\")\n",
    "    \n",
    "    all_pass = all(c < 1e-10 for c in [cond1, cond2, cond3, cond4])\n",
    "    print(f\"  All conditions satisfied: {'✓' if all_pass else '✗'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Moore-Penrose conditions verified for all matrix types!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0752327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 3.2: Pseudoinverse from SVD — From Scratch\n",
    "# ============================================================\n",
    "\n",
    "def pinv_via_svd(A, tol=None):\n",
    "    \"\"\"\n",
    "    Compute Moore-Penrose pseudoinverse via SVD.\n",
    "    \n",
    "    A⁺ = V Σ⁺ Uᵀ where Σ⁺ᵢᵢ = 1/σᵢ for σᵢ > tol, else 0\n",
    "    \n",
    "    Parameters:\n",
    "        A: array of shape (m, n)\n",
    "        tol: threshold for treating singular values as zero\n",
    "             Default: max(m,n) * max(σ) * machine_epsilon\n",
    "    \n",
    "    Returns:\n",
    "        A_pinv: array of shape (n, m)\n",
    "    \"\"\"\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Determine tolerance for numerical rank\n",
    "    if tol is None:\n",
    "        tol = max(A.shape) * s[0] * np.finfo(A.dtype).eps\n",
    "    \n",
    "    # Compute reciprocals for non-tiny singular values\n",
    "    s_inv = np.zeros_like(s)\n",
    "    for i, sigma in enumerate(s):\n",
    "        if sigma > tol:\n",
    "            s_inv[i] = 1.0 / sigma\n",
    "    \n",
    "    # A⁺ = V Σ⁺ Uᵀ\n",
    "    # Since we have Vt (V transposed), we need V = Vt.T\n",
    "    # A⁺ = Vt.T @ diag(s_inv) @ U.T\n",
    "    return (Vt.T * s_inv) @ U.T\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 3.2: Pseudoinverse from SVD — From Scratch\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test our implementation\n",
    "np.random.seed(42)\n",
    "test_matrices = [\n",
    "    (\"Full-rank tall\", np.random.randn(6, 4)),\n",
    "    (\"Full-rank wide\", np.random.randn(4, 6)),\n",
    "    (\"Rank-deficient\", np.random.randn(5, 2) @ np.random.randn(2, 5)),\n",
    "]\n",
    "\n",
    "for name, A in test_matrices:\n",
    "    our_pinv = pinv_via_svd(A)\n",
    "    numpy_pinv = np.linalg.pinv(A)\n",
    "    \n",
    "    error = np.linalg.norm(our_pinv - numpy_pinv) / np.linalg.norm(numpy_pinv)\n",
    "    \n",
    "    print(f\"\\n{name} {A.shape}:\")\n",
    "    print(f\"  Rank: {np.linalg.matrix_rank(A)}\")\n",
    "    print(f\"  ||A⁺_ours - A⁺_numpy|| / ||A⁺_numpy|| = {error:.2e}\")\n",
    "    print(f\"  Match: {'✓' if error < 1e-12 else '✗'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Our SVD-based pseudoinverse matches NumPy!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 3.3: Minimum-Norm Property Demonstration\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 3.3: Minimum-Norm Solution Property\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create an underdetermined system (infinitely many solutions)\n",
    "np.random.seed(42)\n",
    "A = np.random.randn(3, 6)  # 3 equations, 6 unknowns\n",
    "b = np.random.randn(3)\n",
    "\n",
    "# The pseudoinverse solution\n",
    "x_pinv = np.linalg.pinv(A) @ b\n",
    "\n",
    "print(f\"System: A ∈ ℝ^(3×6), b ∈ ℝ³\")\n",
    "print(f\"Underdetermined: infinitely many solutions exist\")\n",
    "print(f\"\\nPseudoinverse solution x⁺ = A⁺b:\")\n",
    "print(f\"  ||x⁺||₂ = {np.linalg.norm(x_pinv):.6f}\")\n",
    "print(f\"  Residual ||Ax⁺ - b||₂ = {np.linalg.norm(A @ x_pinv - b):.2e}\")\n",
    "\n",
    "# Generate many other solutions: x = x_pinv + null space component\n",
    "print(f\"\\n{'-'*50}\")\n",
    "print(\"Generating 1000 random solutions of the form x = x⁺ + z\")\n",
    "print(\"where z is in the null space of A\")\n",
    "\n",
    "# Compute null space basis\n",
    "_, s, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "rank = np.sum(s > 1e-10)\n",
    "null_basis = Vt[rank:].T  # Columns span null space\n",
    "\n",
    "print(f\"Null space dimension: {null_basis.shape[1]}\")\n",
    "\n",
    "norms = []\n",
    "for _ in range(1000):\n",
    "    # Random null space component\n",
    "    coeffs = np.random.randn(null_basis.shape[1]) * np.random.uniform(0.1, 5)\n",
    "    z = null_basis @ coeffs\n",
    "    x_other = x_pinv + z\n",
    "    \n",
    "    # Verify it's still a solution\n",
    "    residual = np.linalg.norm(A @ x_other - b)\n",
    "    assert residual < 1e-10, \"Should still solve Ax = b\"\n",
    "    \n",
    "    norms.append(np.linalg.norm(x_other))\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  ||x⁺||₂ (pseudoinverse) = {np.linalg.norm(x_pinv):.6f}\")\n",
    "print(f\"  min ||x||₂ (random)     = {min(norms):.6f}\")\n",
    "print(f\"  max ||x||₂ (random)     = {max(norms):.6f}\")\n",
    "print(f\"  x⁺ is minimum-norm: {'✓' if np.linalg.norm(x_pinv) <= min(norms) + 1e-10 else '✗'}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(norms, bins=50, alpha=0.7, edgecolor='black', label='Random solutions')\n",
    "ax.axvline(np.linalg.norm(x_pinv), color='red', linewidth=3, \n",
    "           linestyle='--', label=f'||x⁺||₂ = {np.linalg.norm(x_pinv):.4f}')\n",
    "ax.set_xlabel('Solution Norm ||x||₂', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Minimum-Norm Property: x⁺ = A⁺b has smallest norm among all solutions', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Verified: Pseudoinverse gives minimum-norm solution!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d176a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Conditioning & Sensitivity Analysis\n",
    "\n",
    "### 4.1 The Condition Number\n",
    "\n",
    "**Definition (Condition Number):**\n",
    "For a matrix $A$ with positive singular values, the **condition number** is:\n",
    "\n",
    "$$\\kappa(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\|A\\| \\cdot \\|A^{-1}\\|$$\n",
    "\n",
    "For the 2-norm, this is exactly $\\sigma_1 / \\sigma_r$ where $r = \\text{rank}(A)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Sensitivity Interpretation\n",
    "\n",
    "**The Fundamental Question:** If we perturb the data slightly, how much does the solution change?\n",
    "\n",
    "**Theorem (Least-Squares Perturbation Bound):**\n",
    "Let $\\hat{x}$ solve $\\min_x \\|Ax - b\\|_2$ and let $\\hat{x} + \\delta x$ solve the perturbed problem with $A + \\delta A$ and $b + \\delta b$. Then:\n",
    "\n",
    "$$\\frac{\\|\\delta x\\|}{\\|\\hat{x}\\|} \\leq \\kappa(A) \\left( \\frac{\\|\\delta A\\|}{\\|A\\|} + \\frac{\\|\\delta b\\|}{\\|b\\|} \\right) + \\kappa(A)^2 \\frac{\\|\\delta A\\|}{\\|A\\|} \\cdot \\text{(residual terms)}$$\n",
    "\n",
    "**Key Insight:**\n",
    "- **Well-conditioned** ($\\kappa(A) \\approx 1$): Small perturbations → Small changes in solution\n",
    "- **Ill-conditioned** ($\\kappa(A) \\gg 1$): Small perturbations → Potentially huge changes\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Geometric Interpretation\n",
    "\n",
    "Consider solving $Ax = b$ where $A$ has singular values $\\sigma_1, \\sigma_2$:\n",
    "\n",
    "$$x = A^+ b = \\sum_{i=1}^r \\frac{1}{\\sigma_i} \\langle u_i, b \\rangle v_i$$\n",
    "\n",
    "- Components along small $\\sigma_i$ directions get **amplified** by $1/\\sigma_i$\n",
    "- If $\\sigma_{\\min}$ is tiny, noise in those directions explodes\n",
    "\n",
    "**Visual Intuition:**\n",
    "- Well-conditioned: $A$ maps a circle to a circle-ish ellipse (mild distortion)\n",
    "- Ill-conditioned: $A$ maps a circle to a very thin ellipse (extreme distortion)\n",
    "- Inverting: thin ellipse → huge amplification along the squeezed direction\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Why This Matters for ML\n",
    "\n",
    "| Situation | Effect | Symptoms |\n",
    "|-----------|--------|----------|\n",
    "| $\\kappa(X^T X) \\gg 1$ (multicollinearity) | Coefficients unstable | Removing one feature changes all coefficients drastically |\n",
    "| Near-singular Hessian | Ill-conditioned optimization | Slow convergence, zigzagging |\n",
    "| Large condition number | Numerical errors dominate | Different runs give different results |\n",
    "\n",
    "**Rule of Thumb:** \n",
    "- $\\kappa(A) > 10^{12}$ → You're losing ~12 digits of precision\n",
    "- For float64 (16 digits), this leaves ~4 significant digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764296e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 4.1: Geometric Visualization of Conditioning\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 4.1: Geometric Visualization of Conditioning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Unit circle\n",
    "theta = np.linspace(0, 2*np.pi, 200)\n",
    "circle = np.vstack([np.cos(theta), np.sin(theta)])\n",
    "\n",
    "condition_numbers = [1.1, 5, 50]\n",
    "titles = ['Well-Conditioned\\nκ ≈ 1.1', 'Moderate\\nκ = 5', 'Ill-Conditioned\\nκ = 50']\n",
    "\n",
    "for ax, kappa, title in zip(axes, condition_numbers, titles):\n",
    "    # Create matrix with specified condition number\n",
    "    # A = U @ diag(sigma) @ V.T with sigma = [1, 1/kappa]\n",
    "    angle = np.pi/6\n",
    "    U = np.array([[np.cos(angle), -np.sin(angle)], \n",
    "                  [np.sin(angle), np.cos(angle)]])\n",
    "    sigma = np.array([1, 1/kappa])\n",
    "    A = U @ np.diag(sigma) @ U.T\n",
    "    \n",
    "    # Transform circle -> ellipse\n",
    "    ellipse = A @ circle\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(circle[0], circle[1], 'b--', linewidth=2, alpha=0.5, label='Unit circle')\n",
    "    ax.plot(ellipse[0], ellipse[1], 'r-', linewidth=2, label=f'A × (unit circle)')\n",
    "    \n",
    "    # Mark axes\n",
    "    ax.plot([0, U[0,0]*sigma[0]], [0, U[1,0]*sigma[0]], 'g-', linewidth=3, \n",
    "            label=f'σ₁ = {sigma[0]:.2f}')\n",
    "    ax.plot([0, U[0,1]*sigma[1]], [0, U[1,1]*sigma[1]], 'm-', linewidth=3,\n",
    "            label=f'σ₂ = {sigma[1]:.3f}')\n",
    "    \n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=9, loc='upper right')\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel('x₁', fontsize=11)\n",
    "    ax.set_ylabel('x₂', fontsize=11)\n",
    "\n",
    "plt.suptitle('Effect of Condition Number on Geometry: A maps circle → ellipse', \n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: As κ increases, the ellipse becomes thinner.\")\n",
    "print(\"Inverting a thin ellipse amplifies errors in the squeezed direction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 4.2: Error Amplification Experiment\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 4.2: Error Amplification by Condition Number\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def create_matrix_with_condition(m, n, kappa):\n",
    "    \"\"\"Create m×n matrix with specified condition number.\"\"\"\n",
    "    U, _ = np.linalg.qr(np.random.randn(m, m))\n",
    "    V, _ = np.linalg.qr(np.random.randn(n, n))\n",
    "    \n",
    "    rank = min(m, n)\n",
    "    # Singular values: geometric progression from 1 to 1/kappa\n",
    "    sigma = np.logspace(0, -np.log10(kappa), rank)\n",
    "    \n",
    "    S = np.zeros((m, n))\n",
    "    S[:rank, :rank] = np.diag(sigma)\n",
    "    \n",
    "    return U @ S @ V.T\n",
    "\n",
    "# Test various condition numbers\n",
    "condition_numbers = np.logspace(0, 15, 50)  # 1 to 10^15\n",
    "m, n = 50, 30\n",
    "\n",
    "# Storage\n",
    "relative_errors = []\n",
    "\n",
    "for kappa in condition_numbers:\n",
    "    A = create_matrix_with_condition(m, n, kappa)\n",
    "    \n",
    "    # True solution\n",
    "    x_true = np.random.randn(n)\n",
    "    b_exact = A @ x_true\n",
    "    \n",
    "    # Perturbed problem (small noise in b)\n",
    "    noise_level = 1e-10\n",
    "    b_noisy = b_exact + noise_level * np.linalg.norm(b_exact) * np.random.randn(m)\n",
    "    \n",
    "    # Solve perturbed problem\n",
    "    x_computed = np.linalg.lstsq(A, b_noisy, rcond=None)[0]\n",
    "    \n",
    "    # Relative error\n",
    "    rel_err = np.linalg.norm(x_computed - x_true) / np.linalg.norm(x_true)\n",
    "    relative_errors.append(rel_err)\n",
    "\n",
    "# Theoretical bound: error ≈ κ × (perturbation)\n",
    "theoretical_bound = condition_numbers * noise_level\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.loglog(condition_numbers, relative_errors, 'b-', linewidth=2, \n",
    "          label='Observed relative error ||x̂ - x*|| / ||x*||', marker='o', markersize=4)\n",
    "ax.loglog(condition_numbers, theoretical_bound, 'r--', linewidth=2,\n",
    "          label=f'Theoretical bound ≈ κ × {noise_level:.0e}')\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(1.0, color='gray', linestyle=':', alpha=0.7, label='100% error')\n",
    "ax.axhline(1e-16, color='green', linestyle=':', alpha=0.7, label='Machine epsilon')\n",
    "\n",
    "ax.set_xlabel('Condition Number κ(A)', fontsize=12)\n",
    "ax.set_ylabel('Relative Error', fontsize=12)\n",
    "ax.set_title('Error Amplification: Tiny perturbation (10⁻¹⁰) → Large error when κ is large', \n",
    "             fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, which='both', alpha=0.3)\n",
    "ax.set_xlim(1, 1e15)\n",
    "ax.set_ylim(1e-16, 1e4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Key Insight:\")\n",
    "print(f\"  Input perturbation: {noise_level:.0e}\")\n",
    "print(f\"  At κ = 10⁰:  Error ≈ {relative_errors[0]:.2e}\")\n",
    "print(f\"  At κ = 10⁸:  Error ≈ {relative_errors[25]:.2e}\")\n",
    "print(f\"  At κ = 10¹⁵: Error ≈ {relative_errors[-1]:.2e}\")\n",
    "print(\"\\n→ Solution error ≈ κ × data error (first-order approximation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4e6dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The Normal Equations Method\n",
    "\n",
    "### 5.1 Derivation\n",
    "\n",
    "The least-squares problem $\\min_x \\|Ax - b\\|_2^2$ has gradient:\n",
    "\n",
    "$$\\nabla_x \\|Ax - b\\|_2^2 = 2A^T(Ax - b)$$\n",
    "\n",
    "Setting gradient to zero:\n",
    "$$A^T A x = A^T b$$\n",
    "\n",
    "These are the **normal equations**. If $A$ has full column rank, then $A^T A$ is positive definite (thus invertible), giving:\n",
    "\n",
    "$$\\hat{x} = (A^T A)^{-1} A^T b$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 The Condition Number Problem\n",
    "\n",
    "**Critical Issue:**\n",
    "$$\\kappa(A^T A) = \\kappa(A)^2$$\n",
    "\n",
    "**Proof:**\n",
    "If $A = U\\Sigma V^T$ (SVD), then:\n",
    "$$A^T A = V \\Sigma^T \\Sigma V^T = V \\text{diag}(\\sigma_1^2, \\ldots, \\sigma_n^2) V^T$$\n",
    "\n",
    "So:\n",
    "$$\\kappa(A^T A) = \\frac{\\sigma_1^2}{\\sigma_n^2} = \\left(\\frac{\\sigma_1}{\\sigma_n}\\right)^2 = \\kappa(A)^2$$\n",
    "\n",
    "**Consequences:**\n",
    "| $\\kappa(A)$ | $\\kappa(A^T A)$ | Digits Lost |\n",
    "|-------------|-----------------|-------------|\n",
    "| $10^4$ | $10^8$ | ~8 digits |\n",
    "| $10^6$ | $10^{12}$ | ~12 digits |\n",
    "| $10^8$ | $10^{16}$ | All precision lost! |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 When Normal Equations Work Fine\n",
    "\n",
    "Despite the condition number issue, normal equations are often used because:\n",
    "\n",
    "1. **Small, well-conditioned problems**: If $\\kappa(A) < 10^6$, squaring still fits in float64\n",
    "2. **Dense systems**: Cholesky factorization of $A^T A$ is fast ($\\frac{1}{3}n^3$ flops)\n",
    "3. **Symmetric positive definite**: Can use efficient Cholesky instead of general LU\n",
    "\n",
    "**Algorithm (Normal Equations via Cholesky):**\n",
    "```\n",
    "1. Form G = AᵀA           # O(mn²)\n",
    "2. Form c = Aᵀb           # O(mn)\n",
    "3. Cholesky: G = LLᵀ      # O(n³/3)\n",
    "4. Solve Ly = c           # O(n²)\n",
    "5. Solve Lᵀx = y          # O(n²)\n",
    "```\n",
    "\n",
    "Total: $O(mn^2 + n^3/3)$ — often faster than SVD's $O(mn^2 + n^3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 5.1: Normal Equations — From Scratch\n",
    "# ============================================================\n",
    "\n",
    "def solve_normal_equations(A, b):\n",
    "    \"\"\"\n",
    "    Solve least-squares via normal equations with Cholesky.\n",
    "    \n",
    "    Solves: AᵀAx = Aᵀb using Cholesky factorization.\n",
    "    \n",
    "    Parameters:\n",
    "        A: array of shape (m, n) with m >= n and full column rank\n",
    "        b: array of shape (m,)\n",
    "    \n",
    "    Returns:\n",
    "        x: least-squares solution of shape (n,)\n",
    "    \"\"\"\n",
    "    # Form normal equations\n",
    "    ATA = A.T @ A\n",
    "    ATb = A.T @ b\n",
    "    \n",
    "    # Cholesky factorization: AᵀA = LLᵀ\n",
    "    L = np.linalg.cholesky(ATA)\n",
    "    \n",
    "    # Solve Ly = Aᵀb (forward substitution)\n",
    "    y = np.linalg.solve(L, ATb)\n",
    "    \n",
    "    # Solve Lᵀx = y (backward substitution)\n",
    "    x = np.linalg.solve(L.T, y)\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 5.1: Normal Equations Implementation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test on well-conditioned system\n",
    "np.random.seed(42)\n",
    "m, n = 100, 20\n",
    "A = np.random.randn(m, n)\n",
    "x_true = np.random.randn(n)\n",
    "b = A @ x_true + 0.01 * np.random.randn(m)  # Small noise\n",
    "\n",
    "# Solve\n",
    "x_normal = solve_normal_equations(A, b)\n",
    "x_lstsq = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "print(f\"\\nSystem: {m}×{n} matrix\")\n",
    "print(f\"Condition number κ(A) = {np.linalg.cond(A):.2f}\")\n",
    "print(f\"Condition number κ(AᵀA) = {np.linalg.cond(A.T @ A):.2f}\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  ||x_normal - x_lstsq|| = {np.linalg.norm(x_normal - x_lstsq):.2e}\")\n",
    "print(f\"  ||x_normal - x_true|| = {np.linalg.norm(x_normal - x_true):.6f}\")\n",
    "print(f\"  Residual ||Ax - b|| = {np.linalg.norm(A @ x_normal - b):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Normal equations work well for well-conditioned systems\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 5.2: Condition Number Squaring Demonstration\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 5.2: Condition Number Squaring Effect\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "condition_numbers = [1e2, 1e4, 1e6, 1e8]\n",
    "m, n = 50, 20\n",
    "\n",
    "print(f\"\\nComparing κ(A) vs κ(AᵀA):\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'κ(A)':>12} | {'κ(AᵀA)':>15} | {'Ratio':>12}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for kappa in condition_numbers:\n",
    "    A = create_matrix_with_condition(m, n, kappa)\n",
    "    \n",
    "    kappa_A = np.linalg.cond(A)\n",
    "    kappa_ATA = np.linalg.cond(A.T @ A)\n",
    "    \n",
    "    print(f\"{kappa_A:>12.2e} | {kappa_ATA:>15.2e} | {kappa_ATA/kappa_A:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 45)\n",
    "print(\"\\n→ Confirmed: κ(AᵀA) = κ(A)²\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "kappas = np.logspace(1, 8, 30)\n",
    "kappa_squared = []\n",
    "\n",
    "for k in kappas:\n",
    "    A = create_matrix_with_condition(m, n, k)\n",
    "    kappa_squared.append(np.linalg.cond(A.T @ A))\n",
    "\n",
    "ax.loglog(kappas, kappas**2, 'r--', linewidth=2, label='Theoretical: κ(A)²')\n",
    "ax.loglog(kappas, kappa_squared, 'bo', markersize=6, alpha=0.7, label='Measured κ(AᵀA)')\n",
    "\n",
    "ax.set_xlabel('Condition Number κ(A)', fontsize=12)\n",
    "ax.set_ylabel('Condition Number κ(AᵀA)', fontsize=12)\n",
    "ax.set_title('Normal Equations Square the Condition Number', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, which='both', alpha=0.3)\n",
    "\n",
    "# Danger zone\n",
    "ax.axhline(1e16, color='orange', linestyle=':', alpha=0.8)\n",
    "ax.text(1e2, 3e16, 'float64 precision limit', fontsize=10, color='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n⚠️  When κ(A) > 10⁸, normal equations lose all precision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da928f14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. QR Factorization Method\n",
    "\n",
    "### 6.1 The QR Approach\n",
    "\n",
    "For $A = QR$ where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular:\n",
    "\n",
    "$$\\|Ax - b\\|^2 = \\|QRx - b\\|^2 = \\|Rx - Q^T b\\|^2$$\n",
    "\n",
    "(since $\\|Q^T v\\|^2 = \\|v\\|^2$ for orthogonal $Q$)\n",
    "\n",
    "Let $c = Q^T b$. The minimum is achieved when:\n",
    "$$Rx = c$$\n",
    "\n",
    "This is an upper triangular system — solve by **back substitution**!\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Why QR is Numerically Superior\n",
    "\n",
    "**Key Property:** QR avoids forming $A^T A$.\n",
    "\n",
    "The condition number of $R$ equals $\\kappa(A)$ (not $\\kappa(A)^2$!).\n",
    "\n",
    "**Proof sketch:**\n",
    "Since $A = QR$ and $Q$ has orthonormal columns:\n",
    "$$\\kappa(A) = \\kappa(QR) = \\kappa(R)$$\n",
    "(orthogonal transformations preserve singular values)\n",
    "\n",
    "**Stability Comparison:**\n",
    "| Method | Effective Condition Number | Back-Stability |\n",
    "|--------|---------------------------|----------------|\n",
    "| Normal Equations | $\\kappa(A)^2$ | Unstable for ill-conditioned |\n",
    "| QR Factorization | $\\kappa(A)$ | Backward stable |\n",
    "| SVD | $\\kappa(A)$ | Most robust |\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Algorithm\n",
    "\n",
    "**QR Least-Squares Algorithm:**\n",
    "```\n",
    "1. QR factorization: A = QR    # O(2mn² - 2n³/3) via Householder\n",
    "2. Compute c = Qᵀb            # O(mn)\n",
    "3. Back-solve Rx = c          # O(n²)\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- No squaring of condition number\n",
    "- Faster than SVD (no full singular value decomposition)\n",
    "- Numerically stable (especially with Householder reflections)\n",
    "\n",
    "**Disadvantage:**\n",
    "- Requires $A$ to have full column rank (else $R$ is singular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41978fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 6.1: QR Least-Squares — From Scratch\n",
    "# ============================================================\n",
    "\n",
    "def solve_qr_lstsq(A, b):\n",
    "    \"\"\"\n",
    "    Solve least-squares via QR factorization.\n",
    "    \n",
    "    For A = QR, solves: Rx = Qᵀb\n",
    "    \n",
    "    Parameters:\n",
    "        A: array of shape (m, n) with m >= n and full column rank\n",
    "        b: array of shape (m,)\n",
    "    \n",
    "    Returns:\n",
    "        x: least-squares solution of shape (n,)\n",
    "    \"\"\"\n",
    "    # Reduced QR factorization\n",
    "    Q, R = np.linalg.qr(A, mode='reduced')\n",
    "    \n",
    "    # Transform RHS: c = Qᵀb\n",
    "    c = Q.T @ b\n",
    "    \n",
    "    # Back substitution for Rx = c\n",
    "    # (Using built-in triangular solver for efficiency)\n",
    "    x = np.linalg.solve(R, c)\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 6.1: QR Least-Squares Implementation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test on moderately ill-conditioned system\n",
    "np.random.seed(42)\n",
    "m, n = 100, 20\n",
    "kappa = 1e6\n",
    "A = create_matrix_with_condition(m, n, kappa)\n",
    "x_true = np.random.randn(n)\n",
    "b = A @ x_true + 0.01 * np.random.randn(m)\n",
    "\n",
    "# Solve with different methods\n",
    "x_qr = solve_qr_lstsq(A, b)\n",
    "x_lstsq = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "# Compute QR and check R's condition number\n",
    "Q, R = np.linalg.qr(A, mode='reduced')\n",
    "\n",
    "print(f\"\\nSystem: {m}×{n} matrix with κ(A) = {np.linalg.cond(A):.2e}\")\n",
    "print(f\"Condition number κ(R) = {np.linalg.cond(R):.2e}\")\n",
    "print(f\"Note: κ(R) ≈ κ(A), not κ(A)²!\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  ||x_qr - x_lstsq|| = {np.linalg.norm(x_qr - x_lstsq):.2e}\")\n",
    "print(f\"  ||x_qr - x_true|| = {np.linalg.norm(x_qr - x_true):.6f}\")\n",
    "print(f\"  Residual ||Ax - b|| = {np.linalg.norm(A @ x_qr - b):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ QR method maintains conditioning (no squaring!)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5578c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Three Solvers: Complete Implementation & Comparison\n",
    "\n",
    "### 7.1 Implementation Summary\n",
    "\n",
    "We now have three approaches to solve least-squares:\n",
    "\n",
    "| Method | Formula | Effective $\\kappa$ | Complexity |\n",
    "|--------|---------|-------------------|------------|\n",
    "| **SVD** | $x = V\\Sigma^+ U^T b$ | $\\kappa(A)$ | $O(mn^2 + n^3)$ |\n",
    "| **Normal Eq** | $x = (A^T A)^{-1} A^T b$ | $\\kappa(A)^2$ | $O(mn^2 + n^3/3)$ |\n",
    "| **QR** | $x = R^{-1} Q^T b$ | $\\kappa(A)$ | $O(2mn^2 - 2n^3/3)$ |\n",
    "\n",
    "**Recommendation Hierarchy:**\n",
    "1. **Well-conditioned ($\\kappa < 10^6$):** Normal equations (fastest)\n",
    "2. **Moderate conditioning ($10^6 < \\kappa < 10^{12}$):** QR factorization\n",
    "3. **Ill-conditioned or rank-deficient:** SVD (most robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 7.1: Three Solvers — Complete Implementation\n",
    "# ============================================================\n",
    "\n",
    "def solve_lstsq_svd(A, b, tol=None):\n",
    "    \"\"\"Solve least-squares via SVD (most robust).\"\"\"\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    if tol is None:\n",
    "        tol = max(A.shape) * s[0] * np.finfo(A.dtype).eps\n",
    "    \n",
    "    # Pseudoinverse solution\n",
    "    s_inv = np.where(s > tol, 1/s, 0)\n",
    "    return Vt.T @ (s_inv * (U.T @ b))\n",
    "\n",
    "def solve_lstsq_normal(A, b):\n",
    "    \"\"\"Solve least-squares via normal equations (Cholesky).\"\"\"\n",
    "    ATA = A.T @ A\n",
    "    ATb = A.T @ b\n",
    "    L = np.linalg.cholesky(ATA)\n",
    "    y = np.linalg.solve(L, ATb)\n",
    "    return np.linalg.solve(L.T, y)\n",
    "\n",
    "def solve_lstsq_qr(A, b):\n",
    "    \"\"\"Solve least-squares via QR factorization.\"\"\"\n",
    "    Q, R = np.linalg.qr(A, mode='reduced')\n",
    "    return np.linalg.solve(R, Q.T @ b)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 7.1: Three Solvers — Correctness Verification\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "m, n = 100, 30\n",
    "A = np.random.randn(m, n)\n",
    "x_true = np.random.randn(n)\n",
    "b = A @ x_true + 0.01 * np.random.randn(m)\n",
    "\n",
    "# Solve with all three methods\n",
    "x_svd = solve_lstsq_svd(A, b)\n",
    "x_normal = solve_lstsq_normal(A, b)\n",
    "x_qr = solve_lstsq_qr(A, b)\n",
    "x_numpy = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "print(f\"\\nSystem: {m}×{n}, κ(A) = {np.linalg.cond(A):.2f}\")\n",
    "print(f\"\\nSolution Comparison (vs NumPy lstsq):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Method':<15} | {'||x - x_numpy||':>15} | {'||Ax - b||':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'SVD':<15} | {np.linalg.norm(x_svd - x_numpy):>15.2e} | {np.linalg.norm(A @ x_svd - b):>12.6f}\")\n",
    "print(f\"{'Normal Eq':<15} | {np.linalg.norm(x_normal - x_numpy):>15.2e} | {np.linalg.norm(A @ x_normal - b):>12.6f}\")\n",
    "print(f\"{'QR':<15} | {np.linalg.norm(x_qr - x_numpy):>15.2e} | {np.linalg.norm(A @ x_qr - b):>12.6f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n✓ All three methods agree for well-conditioned systems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45485656",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Ill-Conditioned Experiments\n",
    "\n",
    "### 8.1 The Ultimate Test\n",
    "\n",
    "Now we stress-test our solvers on **increasingly ill-conditioned matrices** to see where each method breaks down.\n",
    "\n",
    "**Experimental Setup:**\n",
    "- Matrix size: $1000 \\times 300$ (realistic regression scale)\n",
    "- Condition numbers: $10^0$ to $10^{16}$\n",
    "- Noise level: $10^{-10}$ relative perturbation\n",
    "- Metrics: Solution error, residual, numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 8.1: Comprehensive Ill-Conditioning Experiment\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 8.1: Comprehensive Ill-Conditioning Experiment\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing 1000×300 matrices with various condition numbers...\")\n",
    "print(\"This may take a moment...\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "m, n = 1000, 300\n",
    "condition_numbers = np.logspace(0, 15, 40)\n",
    "noise_level = 1e-10\n",
    "\n",
    "# Storage for results\n",
    "results = {\n",
    "    'kappa': [],\n",
    "    'svd_error': [], 'svd_residual': [],\n",
    "    'normal_error': [], 'normal_residual': [],\n",
    "    'qr_error': [], 'qr_residual': [],\n",
    "}\n",
    "\n",
    "for i, kappa in enumerate(condition_numbers):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processing κ ≈ 10^{np.log10(kappa):.0f}...\")\n",
    "    \n",
    "    # Create ill-conditioned matrix\n",
    "    A = create_matrix_with_condition(m, n, kappa)\n",
    "    \n",
    "    # Ground truth\n",
    "    x_true = np.random.randn(n)\n",
    "    b_exact = A @ x_true\n",
    "    \n",
    "    # Perturbed RHS\n",
    "    b = b_exact + noise_level * np.linalg.norm(b_exact) * np.random.randn(m)\n",
    "    \n",
    "    results['kappa'].append(kappa)\n",
    "    \n",
    "    # SVD solver (always works)\n",
    "    try:\n",
    "        x_svd = solve_lstsq_svd(A, b)\n",
    "        results['svd_error'].append(np.linalg.norm(x_svd - x_true) / np.linalg.norm(x_true))\n",
    "        results['svd_residual'].append(np.linalg.norm(A @ x_svd - b))\n",
    "    except:\n",
    "        results['svd_error'].append(np.nan)\n",
    "        results['svd_residual'].append(np.nan)\n",
    "    \n",
    "    # Normal equations (may fail)\n",
    "    try:\n",
    "        x_normal = solve_lstsq_normal(A, b)\n",
    "        results['normal_error'].append(np.linalg.norm(x_normal - x_true) / np.linalg.norm(x_true))\n",
    "        results['normal_residual'].append(np.linalg.norm(A @ x_normal - b))\n",
    "    except np.linalg.LinAlgError:\n",
    "        results['normal_error'].append(np.nan)\n",
    "        results['normal_residual'].append(np.nan)\n",
    "    \n",
    "    # QR solver\n",
    "    try:\n",
    "        x_qr = solve_lstsq_qr(A, b)\n",
    "        results['qr_error'].append(np.linalg.norm(x_qr - x_true) / np.linalg.norm(x_true))\n",
    "        results['qr_residual'].append(np.linalg.norm(A @ x_qr - b))\n",
    "    except:\n",
    "        results['qr_error'].append(np.nan)\n",
    "        results['qr_residual'].append(np.nan)\n",
    "\n",
    "print(\"\\n✓ Experiment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f752fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 8.2: Visualize Solver Comparison\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 8.2: Solver Comparison Visualization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Solution Error\n",
    "ax1 = axes[0]\n",
    "ax1.loglog(results['kappa'], results['svd_error'], 'b-', linewidth=2, \n",
    "           marker='o', markersize=4, label='SVD', alpha=0.8)\n",
    "ax1.loglog(results['kappa'], results['qr_error'], 'g-', linewidth=2,\n",
    "           marker='s', markersize=4, label='QR', alpha=0.8)\n",
    "ax1.loglog(results['kappa'], results['normal_error'], 'r-', linewidth=2,\n",
    "           marker='^', markersize=4, label='Normal Eq', alpha=0.8)\n",
    "\n",
    "# Reference lines\n",
    "ax1.loglog(results['kappa'], np.array(results['kappa']) * noise_level, 'k--', \n",
    "           linewidth=1.5, alpha=0.7, label='κ × ε (theoretical)')\n",
    "ax1.axhline(1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax1.axhline(1e-16, color='purple', linestyle=':', alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('Condition Number κ(A)', fontsize=12)\n",
    "ax1.set_ylabel('Relative Solution Error ||x̂ - x*|| / ||x*||', fontsize=12)\n",
    "ax1.set_title('Solution Accuracy vs Conditioning', fontsize=13)\n",
    "ax1.legend(fontsize=10, loc='upper left')\n",
    "ax1.grid(True, which='both', alpha=0.3)\n",
    "ax1.set_xlim(1, 1e15)\n",
    "ax1.set_ylim(1e-17, 1e5)\n",
    "\n",
    "# Annotate breakdown regions\n",
    "ax1.axvspan(1e7, 1e8, alpha=0.1, color='orange', label='Normal Eq breaks')\n",
    "ax1.axvspan(1e14, 1e15, alpha=0.1, color='red')\n",
    "ax1.text(3e7, 1e-14, 'Normal Eq\\nbreakdown', fontsize=9, color='orange')\n",
    "ax1.text(3e14, 1e-14, 'QR\\nbreakdown', fontsize=9, color='red')\n",
    "\n",
    "# Plot 2: Residual\n",
    "ax2 = axes[1]\n",
    "ax2.loglog(results['kappa'], results['svd_residual'], 'b-', linewidth=2,\n",
    "           marker='o', markersize=4, label='SVD', alpha=0.8)\n",
    "ax2.loglog(results['kappa'], results['qr_residual'], 'g-', linewidth=2,\n",
    "           marker='s', markersize=4, label='QR', alpha=0.8)\n",
    "ax2.loglog(results['kappa'], results['normal_residual'], 'r-', linewidth=2,\n",
    "           marker='^', markersize=4, label='Normal Eq', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Condition Number κ(A)', fontsize=12)\n",
    "ax2.set_ylabel('Residual ||Ax̂ - b||', fontsize=12)\n",
    "ax2.set_title('Residual vs Conditioning', fontsize=13)\n",
    "ax2.legend(fontsize=10, loc='upper left')\n",
    "ax2.grid(True, which='both', alpha=0.3)\n",
    "ax2.set_xlim(1, 1e15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  1. SVD is most robust across all condition numbers\")\n",
    "print(\"  2. QR matches SVD until very high κ (> 10¹²)\")\n",
    "print(\"  3. Normal Equations break down around κ ≈ 10⁸ (since κ² = 10¹⁶)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 8.3: Summary Table at Key Condition Numbers\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 8.3: Solver Performance Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find indices for specific condition numbers\n",
    "target_kappas = [1e0, 1e4, 1e8, 1e12, 1e15]\n",
    "print(f\"\\n{'κ(A)':>12} | {'SVD Error':>12} | {'QR Error':>12} | {'Normal Error':>14}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for target in target_kappas:\n",
    "    idx = np.argmin(np.abs(np.array(results['kappa']) - target))\n",
    "    k = results['kappa'][idx]\n",
    "    svd_e = results['svd_error'][idx]\n",
    "    qr_e = results['qr_error'][idx]\n",
    "    norm_e = results['normal_error'][idx]\n",
    "    \n",
    "    def fmt(x):\n",
    "        if np.isnan(x):\n",
    "            return \"FAILED\"\n",
    "        elif x > 1:\n",
    "            return f\"{x:.2e} ⚠️\"\n",
    "        else:\n",
    "            return f\"{x:.2e}\"\n",
    "    \n",
    "    print(f\"{k:>12.0e} | {fmt(svd_e):>12} | {fmt(qr_e):>12} | {fmt(norm_e):>14}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\n⚠️  = Error > 100% (solution is meaningless)\")\n",
    "print(\"FAILED = Solver crashed (Cholesky failed, singular matrix)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSIONS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. NORMAL EQUATIONS (κ < 10⁶-10⁸):\n",
    "   - Fastest for well-conditioned problems\n",
    "   - Breaks down when κ(A)² exceeds float64 precision (~10¹⁶)\n",
    "   \n",
    "2. QR FACTORIZATION (κ < 10¹²-10¹⁴):\n",
    "   - Good middle ground: fast and reasonably stable\n",
    "   - Works until κ(A) itself approaches float64 limits\n",
    "   \n",
    "3. SVD (any κ):\n",
    "   - Most robust method\n",
    "   - Handles rank-deficient and severely ill-conditioned cases\n",
    "   - Slowest, but provides additional diagnostic info\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df700b13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Practical ML Context\n",
    "\n",
    "### 9.1 Linear Regression & Multicollinearity\n",
    "\n",
    "In regression with design matrix $X \\in \\mathbb{R}^{n \\times p}$:\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "**Multicollinearity** = high correlation between features → large $\\kappa(X)$.\n",
    "\n",
    "**Symptoms:**\n",
    "- Small changes in data → huge changes in $\\hat{\\beta}$\n",
    "- Coefficients have unexpected signs/magnitudes\n",
    "- Standard errors on coefficients are very large\n",
    "\n",
    "**Solutions:**\n",
    "1. **Ridge Regression:** Add regularization: $(X^T X + \\lambda I)^{-1} X^T y$\n",
    "   - Effectively adds $\\lambda$ to all eigenvalues\n",
    "   - Reduces condition number to $\\frac{\\sigma_1^2 + \\lambda}{\\sigma_n^2 + \\lambda}$\n",
    "   \n",
    "2. **PCA/Feature Selection:** Remove correlated features\n",
    "\n",
    "3. **Use SVD solver:** At minimum, don't use normal equations\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2 Optimization Context\n",
    "\n",
    "**Gradient Descent on Quadratic:**\n",
    "$$f(x) = \\frac{1}{2} x^T A x - b^T x$$\n",
    "\n",
    "Gradient: $\\nabla f = Ax - b$\n",
    "\n",
    "Optimal step size: $\\alpha^* = \\frac{r^T r}{r^T A r}$\n",
    "\n",
    "**Convergence rate:**\n",
    "$$\\|x_{k+1} - x^*\\| \\leq \\left(\\frac{\\kappa - 1}{\\kappa + 1}\\right) \\|x_k - x^*\\|$$\n",
    "\n",
    "**Impact:**\n",
    "| $\\kappa(A)$ | Iterations to halve error |\n",
    "|-------------|--------------------------|\n",
    "| 1 | 0.7 |\n",
    "| 10 | 2.4 |\n",
    "| 100 | 4.6 |\n",
    "| 1000 | 6.9 |\n",
    "| 10000 | 9.2 |\n",
    "\n",
    "**High conditioning → slow convergence → need preconditioning!**\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3 When to Worry\n",
    "\n",
    "| Signal | Action |\n",
    "|--------|--------|\n",
    "| $\\kappa(X) > 30$ | Consider: Is multicollinearity real or spurious? |\n",
    "| $\\kappa(X) > 10^3$ | Strongly consider regularization |\n",
    "| $\\kappa(X) > 10^6$ | Use QR or SVD solver, definitely regularize |\n",
    "| $\\kappa(X) > 10^{12}$ | Problem is essentially rank-deficient, need reformulation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 9.1: Ridge Regression Improves Conditioning\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 9.1: Ridge Regression Improves Conditioning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create ill-conditioned design matrix (multicollinear features)\n",
    "n, p = 200, 50\n",
    "X_base = np.random.randn(n, 10)\n",
    "# Create correlated features\n",
    "X = np.column_stack([X_base] + [X_base @ np.random.randn(10, 10) * 0.1 + np.random.randn(n, 10) * 0.01 for _ in range(4)])\n",
    "\n",
    "print(f\"Design matrix X: {X.shape}\")\n",
    "print(f\"Condition number κ(X): {np.linalg.cond(X):.2e}\")\n",
    "print(f\"Condition number κ(XᵀX): {np.linalg.cond(X.T @ X):.2e}\")\n",
    "\n",
    "# True coefficients and response\n",
    "beta_true = np.random.randn(p)\n",
    "y = X @ beta_true + 0.1 * np.random.randn(n)\n",
    "\n",
    "# Try OLS\n",
    "try:\n",
    "    beta_ols = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    ols_error = np.linalg.norm(beta_ols - beta_true) / np.linalg.norm(beta_true)\n",
    "    print(f\"\\nOLS relative error: {ols_error:.4f}\")\n",
    "except:\n",
    "    print(\"\\nOLS: Cholesky failed!\")\n",
    "\n",
    "# Ridge regression for various λ\n",
    "lambdas = np.logspace(-6, 2, 50)\n",
    "ridge_errors = []\n",
    "condition_numbers = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    XTX_ridge = X.T @ X + lam * np.eye(p)\n",
    "    beta_ridge = np.linalg.solve(XTX_ridge, X.T @ y)\n",
    "    \n",
    "    ridge_errors.append(np.linalg.norm(beta_ridge - beta_true) / np.linalg.norm(beta_true))\n",
    "    condition_numbers.append(np.linalg.cond(XTX_ridge))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Error vs λ\n",
    "ax1 = axes[0]\n",
    "ax1.semilogx(lambdas, ridge_errors, 'b-', linewidth=2)\n",
    "ax1.axhline(ols_error, color='r', linestyle='--', label=f'OLS error = {ols_error:.4f}')\n",
    "best_idx = np.argmin(ridge_errors)\n",
    "ax1.scatter([lambdas[best_idx]], [ridge_errors[best_idx]], color='green', s=100, \n",
    "            zorder=5, label=f'Best λ = {lambdas[best_idx]:.2e}')\n",
    "ax1.set_xlabel('Regularization λ', fontsize=12)\n",
    "ax1.set_ylabel('Relative Error ||β̂ - β*|| / ||β*||', fontsize=12)\n",
    "ax1.set_title('Ridge Regression: Bias-Variance Tradeoff', fontsize=13)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Condition number vs λ\n",
    "ax2 = axes[1]\n",
    "ax2.loglog(lambdas, condition_numbers, 'g-', linewidth=2)\n",
    "ax2.axhline(np.linalg.cond(X.T @ X), color='r', linestyle='--', label='κ(XᵀX) no regularization')\n",
    "ax2.set_xlabel('Regularization λ', fontsize=12)\n",
    "ax2.set_ylabel('Condition Number κ(XᵀX + λI)', fontsize=12)\n",
    "ax2.set_title('Ridge Regularization Reduces Condition Number', fontsize=13)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, which='both', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest ridge λ = {lambdas[best_idx]:.2e} gives error = {ridge_errors[best_idx]:.4f}\")\n",
    "print(f\"This reduces κ(XᵀX + λI) from {np.linalg.cond(X.T @ X):.2e} to {condition_numbers[best_idx]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d707264",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Algorithm Decision Flowchart\n",
    "\n",
    "### 10.1 Choosing a Least-Squares Solver\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────┐\n",
    "│ Need to solve min_x ||Ax - b||²    │\n",
    "└──────────────────┬──────────────────┘\n",
    "                   │\n",
    "                   ▼\n",
    "         ┌─────────────────────┐\n",
    "         │ Is A rank-deficient │\n",
    "         │ or nearly so?       │\n",
    "         └──────────┬──────────┘\n",
    "                    │\n",
    "         ┌──────────┴──────────┐\n",
    "         │ YES                 │ NO\n",
    "         ▼                     ▼\n",
    "   ┌───────────┐      ┌─────────────────┐\n",
    "   │  Use SVD  │      │ What is κ(A)?   │\n",
    "   │  (pinv)   │      └────────┬────────┘\n",
    "   └───────────┘               │\n",
    "                    ┌──────────┴──────────┐\n",
    "                    │                     │\n",
    "              κ < 10⁶               κ > 10⁶\n",
    "                    │                     │\n",
    "                    ▼                     ▼\n",
    "           ┌──────────────┐      ┌──────────────┐\n",
    "           │ Normal Eq    │      │ Is κ < 10¹²? │\n",
    "           │ (fastest)    │      └──────┬───────┘\n",
    "           └──────────────┘             │\n",
    "                              ┌─────────┴─────────┐\n",
    "                              │ YES               │ NO\n",
    "                              ▼                   ▼\n",
    "                       ┌───────────┐       ┌───────────┐\n",
    "                       │ QR factor │       │ Use SVD   │\n",
    "                       │ (stable)  │       │ or reform │\n",
    "                       └───────────┘       └───────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 Quick Reference Card\n",
    "\n",
    "| Situation | Recommended Method | Why |\n",
    "|-----------|-------------------|-----|\n",
    "| Well-conditioned, need speed | Normal Equations + Cholesky | $O(n^3/3)$ |\n",
    "| Moderate conditioning, full rank | QR Factorization | Stable, fast |\n",
    "| Ill-conditioned, full rank | QR or SVD | Avoid κ² |\n",
    "| Rank-deficient | SVD | Handles rank drop |\n",
    "| Need minimum-norm solution | SVD (pseudoinverse) | Theoretical guarantee |\n",
    "| Regularized (Ridge) | Normal Equations | λ improves conditioning |\n",
    "| Sparse system | Iterative methods | Not covered here |\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 NumPy/SciPy Reference\n",
    "\n",
    "```python\n",
    "# Normal equations (don't form AᵀA explicitly in practice!)\n",
    "x = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "\n",
    "# QR factorization\n",
    "x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "\n",
    "# SVD/Pseudoinverse  \n",
    "x = np.linalg.pinv(A) @ b\n",
    "\n",
    "# Or equivalently:\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "x = Vt.T @ (np.where(s > tol, 1/s, 0) * (U.T @ b))\n",
    "\n",
    "# Ridge regression\n",
    "x = np.linalg.solve(A.T @ A + lambda_ * np.eye(n), A.T @ b)\n",
    "\n",
    "# SciPy alternatives (often better)\n",
    "from scipy.linalg import lstsq, solve, cho_solve, cho_factor, qr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ad6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXERCISE 10.1: Decision Flowchart in Action\n",
    "# ============================================================\n",
    "\n",
    "def recommend_solver(A, verbose=True):\n",
    "    \"\"\"\n",
    "    Recommend a least-squares solver based on matrix properties.\n",
    "    \n",
    "    Returns: (recommendation, reason)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    kappa = np.linalg.cond(A)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Matrix analysis: {m}×{n}, rank={rank}, κ={kappa:.2e}\")\n",
    "    \n",
    "    # Check rank deficiency\n",
    "    if rank < min(m, n):\n",
    "        return \"SVD (pinv)\", f\"Rank-deficient: rank={rank} < min(m,n)={min(m,n)}\"\n",
    "    \n",
    "    # Check conditioning\n",
    "    if kappa < 1e6:\n",
    "        return \"Normal Equations\", f\"Well-conditioned: κ={kappa:.2e} < 10⁶\"\n",
    "    elif kappa < 1e12:\n",
    "        return \"QR Factorization\", f\"Moderate conditioning: 10⁶ < κ={kappa:.2e} < 10¹²\"\n",
    "    else:\n",
    "        return \"SVD\", f\"Ill-conditioned: κ={kappa:.2e} > 10¹²\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Exercise 10.1: Automatic Solver Selection\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (\"Random well-conditioned\", np.random.randn(100, 20)),\n",
    "    (\"Ill-conditioned (κ=10⁸)\", create_matrix_with_condition(100, 20, 1e8)),\n",
    "    (\"Rank-deficient\", np.random.randn(100, 5) @ np.random.randn(5, 20)),\n",
    "    (\"Very ill-conditioned (κ=10¹⁴)\", create_matrix_with_condition(100, 20, 1e14)),\n",
    "]\n",
    "\n",
    "print(\"\\nSolver Recommendations:\")\n",
    "print(\"-\" * 70)\n",
    "for name, A in test_cases:\n",
    "    print(f\"\\n{name}:\")\n",
    "    method, reason = recommend_solver(A)\n",
    "    print(f\"  → Recommended: {method}\")\n",
    "    print(f\"     Reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd884742",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary & Key Takeaways\n",
    "\n",
    "### 11.1 Core Concepts Mastered\n",
    "\n",
    "#### A. The Least-Squares Problem\n",
    "$$\\min_x \\|Ax - b\\|_2^2$$\n",
    "- **Geometric View:** Find $x$ such that $Ax$ is the projection of $b$ onto $\\text{Col}(A)$\n",
    "- **Algebraic View:** Solve $A^T(Ax - b) = 0$ (normal equations)\n",
    "- **SVD View:** $\\hat{x} = A^+ b$ where $A^+ = V\\Sigma^+ U^T$\n",
    "\n",
    "#### B. The Moore-Penrose Pseudoinverse\n",
    "- **Definition:** $A^+ = V\\Sigma^+ U^T$ from SVD\n",
    "- **Key Properties:**\n",
    "  1. $AA^+A = A$ (generalized inverse)\n",
    "  2. $A^+AA^+ = A^+$ (reflexive)\n",
    "  3. $(AA^+)^T = AA^+$ (orthogonal projector onto $\\text{Col}(A)$)\n",
    "  4. $(A^+A)^T = A^+A$ (orthogonal projector onto $\\text{Row}(A)$)\n",
    "- **Minimum-Norm:** Among all least-squares solutions, $x = A^+ b$ has smallest $\\|x\\|_2$\n",
    "\n",
    "#### C. Condition Number & Sensitivity\n",
    "- **Definition:** $\\kappa(A) = \\sigma_{\\max}/\\sigma_{\\min}$\n",
    "- **Meaning:** Amplification factor for input perturbations → output errors\n",
    "- **Rule:** Relative error in solution $\\approx \\kappa(A) \\times$ relative error in data\n",
    "\n",
    "#### D. Three Solvers Compared\n",
    "\n",
    "| Method | Effective κ | When to Use | When to Avoid |\n",
    "|--------|-------------|-------------|---------------|\n",
    "| **Normal Equations** | $\\kappa^2$ | κ < 10⁶, need speed | Ill-conditioned |\n",
    "| **QR Factorization** | $\\kappa$ | Full rank, moderate κ | Rank-deficient |\n",
    "| **SVD** | $\\kappa$ | Always works | Too slow for huge problems |\n",
    "\n",
    "---\n",
    "\n",
    "### 11.2 ML Relevance Summary\n",
    "\n",
    "| ML Task | Least-Squares Connection |\n",
    "|---------|-------------------------|\n",
    "| **Linear Regression** | $\\hat{\\beta} = (X^TX)^{-1}X^Ty$ or $X^+y$ |\n",
    "| **Ridge Regression** | $(X^TX + \\lambda I)^{-1}X^Ty$ — regularization improves κ |\n",
    "| **Neural Network Init** | Ill-conditioned layers → vanishing/exploding gradients |\n",
    "| **PCA** | Condition number of covariance matrix affects stability |\n",
    "| **Optimization** | Hessian condition number determines convergence rate |\n",
    "\n",
    "---\n",
    "\n",
    "### 11.3 Mastery Checklist\n",
    "\n",
    "**Theory:**\n",
    "- [ ] I can derive the normal equations from calculus\n",
    "- [ ] I can explain why $A^+ b$ gives the minimum-norm solution\n",
    "- [ ] I understand why $\\kappa(A^TA) = \\kappa(A)^2$ and its consequences\n",
    "- [ ] I can state the four Moore-Penrose conditions\n",
    "\n",
    "**Implementation:**\n",
    "- [ ] I can implement pseudoinverse via SVD from scratch\n",
    "- [ ] I can implement QR-based least-squares solver\n",
    "- [ ] I can implement normal equations solver (with Cholesky)\n",
    "- [ ] I know when each solver will fail/succeed\n",
    "\n",
    "**Application:**\n",
    "- [ ] I can diagnose multicollinearity from condition number\n",
    "- [ ] I know when to use regularization\n",
    "- [ ] I can explain slow gradient descent via conditioning\n",
    "- [ ] I can select the appropriate solver for a given problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143589ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL VERIFICATION: Run All Key Concepts\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL VERIFICATION: Block 4 Complete\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Quick verification of all main concepts\n",
    "print(\"\\n[1] Moore-Penrose Pseudoinverse\")\n",
    "A = np.random.randn(5, 3)\n",
    "A_pinv = np.linalg.pinv(A)\n",
    "conditions = [\n",
    "    np.allclose(A @ A_pinv @ A, A),\n",
    "    np.allclose(A_pinv @ A @ A_pinv, A_pinv),\n",
    "    np.allclose((A @ A_pinv).T, A @ A_pinv),\n",
    "    np.allclose((A_pinv @ A).T, A_pinv @ A)\n",
    "]\n",
    "print(f\"    All 4 Moore-Penrose conditions: {'✓' if all(conditions) else '✗'}\")\n",
    "\n",
    "print(\"\\n[2] Condition Number Squaring\")\n",
    "kappa_A = np.linalg.cond(A)\n",
    "kappa_ATA = np.linalg.cond(A.T @ A)\n",
    "print(f\"    κ(A) = {kappa_A:.2f}, κ(AᵀA) = {kappa_ATA:.2f}\")\n",
    "print(f\"    κ(AᵀA) ≈ κ(A)²: {'✓' if np.isclose(kappa_ATA, kappa_A**2, rtol=0.1) else '✗'}\")\n",
    "\n",
    "print(\"\\n[3] Three Solvers Agreement (well-conditioned)\")\n",
    "m, n = 50, 10\n",
    "A = np.random.randn(m, n)\n",
    "b = np.random.randn(m)\n",
    "x_svd = solve_lstsq_svd(A, b)\n",
    "x_normal = solve_lstsq_normal(A, b)\n",
    "x_qr = solve_lstsq_qr(A, b)\n",
    "agree = np.allclose(x_svd, x_normal) and np.allclose(x_svd, x_qr)\n",
    "print(f\"    All three solvers agree: {'✓' if agree else '✗'}\")\n",
    "\n",
    "print(\"\\n[4] Minimum-Norm Property\")\n",
    "A_wide = np.random.randn(3, 6)\n",
    "b = np.random.randn(3)\n",
    "x_pinv = np.linalg.pinv(A_wide) @ b\n",
    "# Any other solution\n",
    "_, s, Vt = np.linalg.svd(A_wide, full_matrices=True)\n",
    "null_basis = Vt[np.sum(s > 1e-10):].T\n",
    "x_other = x_pinv + null_basis @ np.random.randn(null_basis.shape[1])\n",
    "is_min_norm = np.linalg.norm(x_pinv) <= np.linalg.norm(x_other) + 1e-10\n",
    "print(f\"    x⁺ has minimum norm: {'✓' if is_min_norm else '✗'}\")\n",
    "\n",
    "print(\"\\n[5] Error Amplification\")\n",
    "A_ill = create_matrix_with_condition(50, 20, 1e8)\n",
    "x_true = np.random.randn(20)\n",
    "b_exact = A_ill @ x_true\n",
    "noise = 1e-10 * np.linalg.norm(b_exact) * np.random.randn(50)\n",
    "x_computed = np.linalg.lstsq(A_ill, b_exact + noise, rcond=None)[0]\n",
    "rel_err = np.linalg.norm(x_computed - x_true) / np.linalg.norm(x_true)\n",
    "expected = 1e8 * 1e-10  # κ × noise\n",
    "amplified = rel_err > 1e-10  # Error larger than input noise\n",
    "print(f\"    κ ≈ 10⁸, noise = 10⁻¹⁰ → error ≈ {rel_err:.2e}\")\n",
    "print(f\"    Error amplified by κ: {'✓' if amplified else '✗'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL CONCEPTS VERIFIED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "BLOCK 4 COMPLETE: Pseudoinverse, Least-Squares & Conditioning\n",
    "\n",
    "You have mastered:\n",
    "• Moore-Penrose pseudoinverse from SVD\n",
    "• Condition number and sensitivity analysis  \n",
    "• Three least-squares solvers with tradeoffs\n",
    "• When and why normal equations fail\n",
    "• Practical implications for ML/regression\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
